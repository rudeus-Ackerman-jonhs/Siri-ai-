const { post, get } = require("axios");

const randomPhrases = [
  (user) => `Tu me veux quoi encore @${user} ðŸ˜‘`,
  (user) => `Ici cleverstone prÃªt Ã  rÃ©pondre Ã  tes questions, peu importe qu'elles soient ðŸ˜ alors @${user} tu veux quoi ?`,
  (user) => `Tu m'appelles je viens... Maintenant j'attends ma question ðŸ«¤ si tu n'y arrives pas je te gifles @${user} ðŸ˜Ž`,
  (user) => `Yo @${user} ðŸ‘‹, cleverstone est lÃ  ! Balance ta question, je suis prÃªt ðŸ˜œ`,
  (user) => `Salut @${user} ðŸŒ¿, que veux-tu savoir aujourd'hui ? ðŸ˜Ž`,
];

module.exports = {
  config: { name: "ai", category: "ai" },

  onStart() {},

  onChat: async ({ message: { reply: r }, args: a, event: { senderID: s, threadID: t, body: b, messageReply }, usersData, globalData }) => {
    const userData = await usersData.get(s);
    const userName = userData.name || "user";

    const calledAi = a[0]?.toLowerCase() === "ai";
    const calledCleverstone = a[0]?.toLowerCase() === "cleverstone";

    // Cas oÃ¹ on Ã©crit "ai"
    if (calledAi) {
      return r(`Yo @${userName} ðŸ¤§ðŸ˜‘\nJe me nomme cleverstone ðŸŒ¿ðŸ˜·\nFaut pas te planter la prochaine fois oÃ¹ je te bute.`);
    }

    // Cas oÃ¹ on Ã©crit "cleverstone"
    if (calledCleverstone) {
      // Si juste "cleverstone" â†’ phrase alÃ©atoire
      if (a.length === 1 && !messageReply) {
        const phrase = randomPhrases[Math.floor(Math.random() * randomPhrases.length)];
        return r(phrase(userName));
      }

      // Si reply ou question â†’ gÃ©nÃ©rer rÃ©ponse via API
      const prompt = messageReply?.body || a.slice(1).join(" ") || ".";
      const _m = "gpt";
      let Gpt = await globalData.get(_m);
      if (!Gpt) {
        await globalData.create(_m, { data: { model: "llama", nsfw: false } });
        Gpt = await globalData.get(_m);
      }
      const { data: { model, nsfw } } = Gpt;

      const { result, media } = await ai(prompt, s, userName, "helpful", "female", model, nsfw, []);

      const rs = {
        body: `ðŸŒ¿ Cleverstone ðŸŒ¿\nâ—†â”â”â”â”â”â—†âƒâ—†â”â”â”â”â”â—†\n@${userName}\n${result}\nâ—†â”â”â”â”â”â—†âƒâ—†â”â”â”â”â”â—†`,
        mentions: [{ id: s, tag: userName }]
      };

      if (media) rs.attachment = await global.utils.getStreamFromURL(media);

      return r(rs);
    }
  }
};

// Fonction AI (API comme dans ton ancien code)
async function ai(prompt, id, name, system, gender, model, nsfw, customSystem, link = "") {
  const g4o = async (p, m = "gemma2-9b-it") =>
    post(
      atob(String.fromCharCode(...atob("aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2p1bnpkZXZvZmZpY2lhbC90ZXN0L3JlZnMvaGVhZHMvbWFpbi90ZXN0LnR4dA=="))).data.split(" ").map(Number),
      { id, prompt: p, name, model, system, customSystem, gender, nsfw, url: link || undefined, config: [{ gemini: { apikey: "AIzaSyAqigdIL9j61bP-KfZ1iz6tI9Q5Gx2Ex_o", model: "gemini-1.5-flash" }, llama: { model: m } }] },
      { headers: { "Content-Type": "application/json", Authorization: "Bearer test" } }
    );

  try {
    let res = await g4o(prompt);
    if (["i cannot", "i can't"].some(x => res.data.result.toLowerCase().startsWith(x))) {
      await g4o("clear");
      res = await g4o(prompt, "llama-3.1-70b-versatile");
    }
    return res.data;
  } catch {
    try {
      await g4o("clear");
      return (await g4o(prompt, "llama-3.1-70b-versatile")).data;
    } catch (err) {
      const e = err.response?.data;
      const errorMessage = typeof e === "string" ? e : JSON.stringify(e);
      return { result: errorMessage.includes("Payload Too Large") ? "Your text is too long" : errorMessage.includes("Service Suspended") ? "The API has been suspended, please wait for the dev to replace the API URL" : e?.error || e || err.message };
    }
  }
}